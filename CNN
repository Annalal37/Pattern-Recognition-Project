# -*- coding: utf-8 -*-

import time
import keras
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten

batch_size = 32 #train 32 samples each time
num_classes = 10 #total number of classes is 10
nb_epoch = 20 #total training times

#load data
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# data pre-processing
#normalize data from images and labels respectively
x_train = x_train.reshape(-1, 28, 28, 1) / 255.
x_test = x_test.reshape(-1, 28, 28, 1) / 255.
y_train = np_utils.to_categorical(y_train, num_classes=10)
y_test = np_utils.to_categorical(y_test, num_classes=10)


######################################################################
model = Sequential()

#first convolutional layer
model.add(Convolution2D(
    batch_input_shape=(None, 28, 28, 1), #input types of parameter
    filters= 16,  #10-60  NODES
    kernel_size=[5,5],
    strides=1,
    padding='same',
))

#max pooling layer
model.add(MaxPooling2D(
    pool_size=2,#default:2
    strides=2,
    padding='same',
))

#second convolutional layer
model.add(Convolution2D(
    filters=50,
    kernel_size=[5,5],
    strides=1,
    padding='same',
))


#fully connected layer (default:20-50-500-10)
model.add(Activation('relu'))
model.add(Flatten())#flat it

model.add(Dense(20)) #the first layer of the fully connected layer
model.add(Activation('relu'))

model.add(Dense(50)) #the second layer of the fully connected layer
model.add(Activation('relu'))

model.add(Dense(500)) #the third layer of the fully connected layer
model.add(Activation('relu'))

model.add(Dense(10)) #the output layer of the fully connected layer
model.add(Activation('softmax'))

#######################################################

#choose adam to be the optimizer
adam = keras.optimizers.Adam(lr=1e-4)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
start = time.time()
h = model.fit(x_train, y_train, epochs=nb_epoch, batch_size=batch_size, validation_data=(x_test, y_test),
              shuffle=True, verbose=2)

#save model to my own folder
model.save('E:/Pattern_Recognition_model/MNIST_MODEL.h5')

#print total time to complete the process
print('@ Total Time Spent: %.2f seconds' % (time.time() - start))

#visualization
def plot_acc_loss(h, nb_epoch):
    acc, loss, val_acc, val_loss = h.history['acc'], h.history['loss'], h.history['val_acc'], h.history['val_loss']
    plt.figure(figsize=(15, 5))
    plt.subplot(121)
    plt.plot(range(nb_epoch), acc, label='Train')
    plt.plot(range(nb_epoch), val_acc, label='Test')
    plt.title('Accuracy over ' + str(nb_epoch) + ' Epochs', size=15)
    plt.legend()
    plt.grid(True)
    plt.subplot(122)
    plt.plot(range(nb_epoch), loss, label='Train')
    plt.plot(range(nb_epoch), val_loss, label='Test')
    plt.title('Loss over ' + str(nb_epoch) + ' Epochs', size=15)
    plt.legend()
    plt.grid(True)
    plt.show()

#visualization
plot_acc_loss(h, nb_epoch)

#calculate and print loss and accuracy of the training data and testing data
loss, accuracy = model.evaluate(x_train, y_train, verbose=0)
print("Training Accuracy = %.2f %%     loss = %f" % (accuracy * 100, loss))
loss, accuracy = model.evaluate(x_test, y_test, verbose=0)
print("Testing Accuracy = %.2f %%    loss = %f" % (accuracy * 100, loss))
###############################################################################3
